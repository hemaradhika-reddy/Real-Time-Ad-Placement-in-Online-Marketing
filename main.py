{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "class AdData:\n",
        "    def __init__(self):\n",
        "        # Ad categories\n",
        "        self.categories = ['Electronics', 'Fashion', 'Food', 'Travel', 'Entertainment']\n",
        "\n",
        "        # User demographic features\n",
        "        self.age_groups = ['18-24', '25-34', '35-44', '45-54', '55+']\n",
        "        self.interests = ['Tech', 'Sports', 'Fashion', 'Food', 'Travel', 'Gaming']\n",
        "        self.device_types = ['Mobile', 'Desktop', 'Tablet']\n",
        "        self.time_slots = ['Morning', 'Afternoon', 'Evening', 'Night']\n",
        "\n",
        "    def generate_ads(self, n_ads=20):\n",
        "        \"\"\"Generate ad inventory with characteristics\"\"\"\n",
        "        ads = []\n",
        "        for i in range(n_ads):\n",
        "            ad = {\n",
        "                'ad_id': f'AD_{i:03d}',\n",
        "                'category': random.choice(self.categories),\n",
        "                'base_ctr': round(random.uniform(0.01, 0.05), 4),\n",
        "                'base_conversion_rate': round(random.uniform(0.1, 0.3), 4),\n",
        "                'cost_per_click': round(random.uniform(0.5, 2.0), 2),\n",
        "                'target_age_groups': random.sample(self.age_groups, random.randint(1, 3)),\n",
        "                'target_interests': random.sample(self.interests, random.randint(1, 4)),\n",
        "                'preferred_devices': random.sample(self.device_types, random.randint(1, 3)),\n",
        "                'best_performing_times': random.sample(self.time_slots, random.randint(1, 2))\n",
        "            }\n",
        "            ads.append(ad)\n",
        "        return pd.DataFrame(ads)\n",
        "\n",
        "    def generate_user_traffic(self, n_users=1000):\n",
        "        \"\"\"Generate user traffic data\"\"\"\n",
        "        users = []\n",
        "        current_time = datetime.now()\n",
        "\n",
        "        for i in range(n_users):\n",
        "            timestamp = current_time + timedelta(minutes=random.randint(0, 1440))\n",
        "            time_of_day = self.get_time_slot(timestamp.hour)\n",
        "\n",
        "            user = {\n",
        "                'user_id': f'USER_{i:04d}',\n",
        "                'age_group': random.choice(self.age_groups),\n",
        "                'interests': random.sample(self.interests, random.randint(1, 3)),\n",
        "                'device': random.choice(self.device_types),\n",
        "                'time_of_day': time_of_day,\n",
        "                'session_duration': random.randint(1, 30),\n",
        "                'historical_ctr': round(random.uniform(0.01, 0.08), 4),\n",
        "                'timestamp': timestamp\n",
        "            }\n",
        "            users.append(user)\n",
        "        return pd.DataFrame(users)\n",
        "\n",
        "    def get_time_slot(self, hour):\n",
        "        if 6 <= hour < 12:\n",
        "            return 'Morning'\n",
        "        elif 12 <= hour < 17:\n",
        "            return 'Afternoon'\n",
        "        elif 17 <= hour < 22:\n",
        "            return 'Evening'\n",
        "        else:\n",
        "            return 'Night'\n",
        "\n",
        "    def calculate_relevance_score(self, user, ad):\n",
        "        \"\"\"Calculate relevance score between user and ad\"\"\"\n",
        "        score = 0\n",
        "\n",
        "        # Age group match\n",
        "        if user['age_group'] in ad['target_age_groups']:\n",
        "            score += 0.3\n",
        "\n",
        "        # Interest match\n",
        "        user_interests = set(user['interests'])\n",
        "        ad_interests = set(ad['target_interests'])\n",
        "        interest_overlap = len(user_interests.intersection(ad_interests))\n",
        "        score += 0.3 * (interest_overlap / max(len(user_interests), len(ad_interests)))\n",
        "\n",
        "        # Device match\n",
        "        if user['device'] in ad['preferred_devices']:\n",
        "            score += 0.2\n",
        "\n",
        "        # Time match\n",
        "        if user['time_of_day'] in ad['best_performing_times']:\n",
        "            score += 0.2\n",
        "\n",
        "        return score"
      ],
      "metadata": {
        "id": "ydW03i74R75e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_and_visualize_data():\n",
        "    # Initialize AdData\n",
        "    ad_data = AdData()\n",
        "\n",
        "    # Generate sample data\n",
        "    ads_df = ad_data.generate_ads(n_ads=50)\n",
        "    users_df = ad_data.generate_user_traffic(n_users=5000)\n",
        "\n",
        "    # Print sample of ad data\n",
        "    print(\"\\n=== Sample Ad Data ===\")\n",
        "    print(ads_df[['ad_id', 'category', 'base_ctr', 'base_conversion_rate', 'cost_per_click']].head())\n",
        "\n",
        "    # Print sample of user data\n",
        "    print(\"\\n=== Sample User Data ===\")\n",
        "    print(users_df[['user_id', 'age_group', 'device', 'time_of_day', 'historical_ctr']].head())\n",
        "\n",
        "    # Visualize distributions\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Ad Categories\n",
        "    plt.subplot(2, 2, 1)\n",
        "    sns.countplot(data=ads_df, x='category')\n",
        "    plt.title('Distribution of Ad Categories')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Plot 2: Base CTR Distribution\n",
        "    plt.subplot(2, 2, 2)\n",
        "    sns.histplot(data=ads_df, x='base_ctr', bins=20)\n",
        "    plt.title('Distribution of Base CTR')\n",
        "\n",
        "    # Plot 3: User Age Groups\n",
        "    plt.subplot(2, 2, 3)\n",
        "    sns.countplot(data=users_df, x='age_group')\n",
        "    plt.title('Distribution of User Age Groups')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Plot 4: Time of Day Distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    sns.countplot(data=users_df, x='time_of_day')\n",
        "    plt.title('Distribution of User Activity Time')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return ads_df, users_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Generate and visualize data\n",
        "    ads_df, users_df = generate_and_visualize_data()"
      ],
      "metadata": {
        "id": "kzj7ECajSHBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdPlacementEnvironment:\n",
        "    def __init__(self, n_slots=3):\n",
        "        self.ad_data = AdData()\n",
        "        self.n_slots = n_slots\n",
        "        self.age_groups = self.ad_data.age_groups\n",
        "        self.interests = self.ad_data.interests\n",
        "        self.device_types = self.ad_data.device_types\n",
        "        self.time_slots = self.ad_data.time_slots\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment with new ads and user\"\"\"\n",
        "        self.ads_df = self.ad_data.generate_ads()\n",
        "        self.current_user = self.ad_data.generate_user_traffic(1).iloc[0]\n",
        "        self.placed_ads = []\n",
        "\n",
        "        # State: user features + available ad slots\n",
        "        state = self._get_state()\n",
        "        return state\n",
        "\n",
        "    def _get_state(self):\n",
        "        \"\"\"Convert current situation to state vector\"\"\"\n",
        "        user_features = [\n",
        "            self.age_groups.index(self.current_user['age_group']) / len(self.age_groups),\n",
        "            len(self.current_user['interests']) / len(self.interests),\n",
        "            self.device_types.index(self.current_user['device']) / len(self.device_types),\n",
        "            self.time_slots.index(self.current_user['time_of_day']) / len(self.time_slots),\n",
        "            self.current_user['historical_ctr']\n",
        "        ]\n",
        "\n",
        "        # Add placement history\n",
        "        placement_features = [0] * self.n_slots\n",
        "        for i, ad_id in enumerate(self.placed_ads):\n",
        "            if i < len(placement_features):\n",
        "                placement_features[i] = 1\n",
        "\n",
        "        return np.array(user_features + placement_features)\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Execute one step in the environment\"\"\"\n",
        "        if len(self.placed_ads) >= self.n_slots:\n",
        "            return self._get_state(), 0, True, {}\n",
        "\n",
        "        # Select ad\n",
        "        selected_ad = self.ads_df.iloc[action]\n",
        "        self.placed_ads.append(selected_ad['ad_id'])\n",
        "\n",
        "        # Calculate reward based on relevance and expected performance\n",
        "        relevance = self.ad_data.calculate_relevance_score(self.current_user, selected_ad)\n",
        "        expected_ctr = selected_ad['base_ctr'] * (1 + relevance)\n",
        "        expected_conversion = selected_ad['base_conversion_rate'] * (1 + relevance)\n",
        "        reward = expected_ctr * expected_conversion * selected_ad['cost_per_click']\n",
        "\n",
        "        done = len(self.placed_ads) >= self.n_slots\n",
        "\n",
        "        return self._get_state(), reward, done, {\n",
        "            'ad_id': selected_ad['ad_id'],\n",
        "            'relevance': relevance,\n",
        "            'expected_ctr': expected_ctr,\n",
        "            'expected_conversion': expected_conversion,\n",
        "            'reward': reward\n",
        "        }"
      ],
      "metadata": {
        "id": "vyTGmLgqSQlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQNNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Networks\n",
        "        self.policy_net = DQNNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_net = DQNNetwork(state_size, action_size).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        # Training parameters\n",
        "        self.learning_rate = 1e-4\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.batch_size = 64\n",
        "        self.target_update = 10\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state)\n",
        "            return q_values.argmax().item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "        next_q_values = self.target_net(next_states).max(1)[0].detach()\n",
        "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
        "\n",
        "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())"
      ],
      "metadata": {
        "id": "e-4dUEPvUu-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(ActorCritic, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Actor head\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Critic head\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared = self.shared(x)\n",
        "        return self.actor(shared), self.critic(shared)\n",
        "\n",
        "class A2CAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = ActorCritic(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.entropy_beta = 0.01\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action_probs, _ = self.model(state)\n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        action = action_dist.sample()\n",
        "        return action.item(), action_probs.squeeze()[action.item()].item()\n",
        "\n",
        "    def update(self, states, actions, rewards, next_states, dones):\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Get action probabilities and state values\n",
        "        action_probs, state_values = self.model(states)\n",
        "        _, next_state_values = self.model(next_states)\n",
        "\n",
        "        # Calculate advantages\n",
        "        next_state_values = next_state_values.squeeze()\n",
        "        state_values = state_values.squeeze()\n",
        "\n",
        "        # TD targets\n",
        "        td_targets = rewards + self.gamma * next_state_values * (1 - dones)\n",
        "        td_errors = td_targets - state_values\n",
        "\n",
        "        # Actor loss\n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        log_probs = action_dist.log_prob(actions)\n",
        "        actor_loss = -(log_probs * td_errors.detach()).mean()\n",
        "\n",
        "        # Critic loss\n",
        "        critic_loss = td_errors.pow(2).mean()\n",
        "\n",
        "        # Entropy loss for exploration\n",
        "        entropy_loss = -self.entropy_beta * action_dist.entropy().mean()\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = actor_loss + critic_loss + entropy_loss\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()"
      ],
      "metadata": {
        "id": "Sko1OfuNU4AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PPONetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(PPONetwork, self).__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Actor head\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        # Critic head\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shared = self.shared(x)\n",
        "        return self.actor(shared), self.critic(shared)\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = PPONetwork(state_size, action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=3e-4)\n",
        "\n",
        "        # PPO parameters\n",
        "        self.clip_epsilon = 0.2\n",
        "        self.gamma = 0.99\n",
        "        self.gae_lambda = 0.95\n",
        "        self.value_coef = 0.5\n",
        "        self.entropy_coef = 0.01\n",
        "        self.ppo_epochs = 10\n",
        "        self.batch_size = 64\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        action_probs, _ = self.model(state)\n",
        "        action_dist = torch.distributions.Categorical(action_probs)\n",
        "        action = action_dist.sample()\n",
        "        return action.item(), action_probs.squeeze()[action.item()].item()\n",
        "\n",
        "    def compute_gae(self, rewards, values, next_values, dones):\n",
        "        advantages = []\n",
        "        gae = 0\n",
        "\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            delta = rewards[t] + self.gamma * next_values[t] * (1 - dones[t]) - values[t]\n",
        "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae\n",
        "            advantages.insert(0, gae)\n",
        "\n",
        "        return torch.FloatTensor(advantages).to(self.device)\n",
        "\n",
        "    def update(self, states, actions, old_probs, rewards, next_states, dones):\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        old_probs = torch.FloatTensor(old_probs).to(self.device)\n",
        "\n",
        "        # Compute advantages and returns\n",
        "        action_probs, values = self.model(states)\n",
        "        _, next_values = self.model(torch.FloatTensor(next_states).to(self.device))\n",
        "\n",
        "        values = values.squeeze()\n",
        "        next_values = next_values.squeeze()\n",
        "\n",
        "        advantages = self.compute_gae(rewards, values.detach().cpu().numpy(),\n",
        "                                    next_values.detach().cpu().numpy(), dones)\n",
        "        returns = advantages + values.detach()\n",
        "\n",
        "        # PPO update\n",
        "        for _ in range(self.ppo_epochs):\n",
        "            # Generate random mini-batches\n",
        "            indices = torch.randperm(len(states))\n",
        "            for start_idx in range(0, len(states), self.batch_size):\n",
        "                batch_indices = indices[start_idx:start_idx + self.batch_size]\n",
        "\n",
        "                batch_states = states[batch_indices]\n",
        "                batch_actions = actions[batch_indices]\n",
        "                batch_old_probs = old_probs[batch_indices]\n",
        "                batch_advantages = advantages[batch_indices]\n",
        "                batch_returns = returns[batch_indices]\n",
        "\n",
        "                # Get current action probabilities and values\n",
        "                new_action_probs, new_values = self.model(batch_states)\n",
        "                new_values = new_values.squeeze()\n",
        "\n",
        "                # Calculate ratio and clipped ratio\n",
        "                action_dist = torch.distributions.Categorical(new_action_probs)\n",
        "                new_probs = action_dist.log_prob(batch_actions).exp()\n",
        "                ratio = new_probs / batch_old_probs\n",
        "\n",
        "                # Clipped PPO objective\n",
        "                surr1 = ratio * batch_advantages\n",
        "                surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * batch_advantages\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                # Value loss\n",
        "                value_loss = 0.5 * (batch_returns - new_values).pow(2).mean()\n",
        "\n",
        "                # Entropy bonus\n",
        "                entropy_loss = -self.entropy_coef * action_dist.entropy().mean()\n",
        "\n",
        "                # Total loss\n",
        "                total_loss = actor_loss + self.value_coef * value_loss + entropy_loss\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "        return total_loss.item()"
      ],
      "metadata": {
        "id": "Ih5TK9LcVIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedMetrics:\n",
        "    def __init__(self):\n",
        "        self.episode_rewards = []\n",
        "        self.ctr_values = []\n",
        "        self.relevance_scores = []\n",
        "        self.conversion_rates = []\n",
        "        self.exploration_vs_exploitation = []\n",
        "        self.success_rates = []\n",
        "        self.placement_history = []\n",
        "\n",
        "    def update_metrics(self, reward, ctr, relevance, conversion_rate, exploration, success_rate,placement_history):\n",
        "        self.episode_rewards.append(reward)\n",
        "        self.ctr_values.append(ctr)\n",
        "        self.relevance_scores.append(relevance)\n",
        "        self.conversion_rates.append(conversion_rate)\n",
        "        self.exploration_vs_exploitation.append(exploration)\n",
        "        self.success_rates.append(success_rate)\n",
        "        self.placemet_history.append(placement_history)\n",
        "\n",
        "\n",
        "    def add_episode_data(self, episode_info):\n",
        "        \"\"\"Add data from one episode\"\"\"\n",
        "        self.episode_rewards.append(sum(info['reward'] for info in episode_info))\n",
        "        self.relevance_scores.extend(info['relevance'] for info in episode_info)\n",
        "        self.ctr_values.extend(info['expected_ctr'] for info in episode_info)\n",
        "        self.conversion_rates.extend(info['expected_conversion'] for info in episode_info)\n",
        "        self.placement_history.extend(info['ad_id'] for info in episode_info)\n",
        "\n",
        "        # Ensure that exploration rate is added correctly\n",
        "        exploration_rate = episode_info[-1].get('exploration_rate', 0.0)\n",
        "        self.exploration_vs_exploitation.append(exploration_rate)\n",
        "\n",
        "        # Success rate calculation\n",
        "        success_count = sum(1 for info in episode_info if info.get('success', False))\n",
        "        self.success_rates.append(success_count / len(episode_info))\n",
        "\n",
        "\n",
        "    def calculate_metrics(self):\n",
        "        \"\"\"Calculate comprehensive metrics\"\"\"\n",
        "        metrics = {\n",
        "            'avg_episode_reward': np.mean(self.episode_rewards),\n",
        "            'std_episode_reward': np.std(self.episode_rewards),\n",
        "            'max_episode_reward': np.max(self.episode_rewards),\n",
        "            'min_episode_reward': np.min(self.episode_rewards),\n",
        "            'avg_relevance': np.mean(self.relevance_scores),\n",
        "            'avg_ctr': np.mean(self.ctr_values),\n",
        "            'avg_conversion': np.mean(self.conversion_rates),\n",
        "            'avg_success_rate': np.mean(self.success_rates),  # Added success rate metric\n",
        "            'unique_ads_used': len(set(self.placement_history)),\n",
        "            'total_episodes': len(self.episode_rewards),\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    def plot_metrics(self):\n",
        "        \"\"\"Create visualization of metrics\"\"\"\n",
        "        plt.figure(figsize=(20, 10))\n",
        "\n",
        "        # Plot 1: Episode Rewards\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.plot(self.episode_rewards)\n",
        "        plt.title('Episode Rewards Over Time')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Total Reward')\n",
        "\n",
        "        # Plot 2: Relevance Distribution\n",
        "        plt.subplot(2, 3, 2)\n",
        "        sns.histplot(self.relevance_scores, bins=20)\n",
        "        plt.title('Distribution of Relevance Scores')\n",
        "\n",
        "        # Plot 3: CTR vs Conversion Rate\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.scatter(self.ctr_values, self.conversion_rates, alpha=0.5)\n",
        "        plt.title('CTR vs Conversion Rate')\n",
        "        plt.xlabel('CTR')\n",
        "        plt.ylabel('Conversion Rate')\n",
        "\n",
        "        # Plot 4: Ad Placement Distribution\n",
        "        plt.subplot(2, 3, 4)\n",
        "        ad_counts = pd.Series(self.placement_history).value_counts()\n",
        "        sns.barplot(x=ad_counts.index[:10], y=ad_counts.values[:10])\n",
        "        plt.title('Top 10 Most Used Ads')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Plot 5: Success Rate Over Episodes\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.plot(self.success_rates)\n",
        "        plt.title('Success Rate Over Episodes')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Success Rate')\n",
        "\n",
        "        # Plot 6: Moving Average of Rewards\n",
        "        plt.subplot(2, 3, 6)\n",
        "        window_size = max(1, len(self.episode_rewards) // 20)\n",
        "        moving_avg = pd.Series(self.episode_rewards).rolling(window=window_size).mean()\n",
        "        plt.plot(moving_avg)\n",
        "        plt.title(f'Moving Average of Rewards (window={window_size})')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Reward')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print detailed summary of metrics\"\"\"\n",
        "        metrics = self.calculate_metrics()\n",
        "        print(\"\\n=== Performance Summary ===\")\n",
        "        for key, value in metrics.items():\n",
        "            print(f\"{key}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\n=== Distribution Statistics ===\")\n",
        "        print(\"\\nReward Distribution:\")\n",
        "        print(pd.Series(self.episode_rewards).describe())\n",
        "\n",
        "        print(\"\\nRelevance Score Distribution:\")\n",
        "        print(pd.Series(self.relevance_scores).describe())\n",
        "        print(\"\\nCTR Distribution:\")\n",
        "        print(pd.Series(self.ctr_values).describe())\n",
        "        print(\"\\nConversion Rate Distribution:\")\n",
        "        print(pd.Series(self.conversion_rates).describe())\n"
      ],
      "metadata": {
        "id": "94oxtxIKWlnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agents(episodes=500):\n",
        "    # Initialize environment\n",
        "    env = AdPlacementEnvironment(n_slots=3)\n",
        "    state_size = len(env.reset())\n",
        "    action_size = len(env.ads_df)\n",
        "\n",
        "    # Initialize agents\n",
        "    dqn_agent = DQNAgent(state_size, action_size)\n",
        "    a2c_agent = A2CAgent(state_size, action_size)\n",
        "    ppo_agent = PPOAgent(state_size, action_size)\n",
        "\n",
        "    # Metrics trackers\n",
        "    dqn_metrics = AdvancedMetrics()\n",
        "    a2c_metrics = AdvancedMetrics()\n",
        "    ppo_metrics = AdvancedMetrics()\n",
        "# Training loop for each agent\n",
        "    def train_agent(agent, metrics_tracker, agent_name):\n",
        "        print(f\"\\n=== Training {agent_name} Agent ===\")\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_info = []\n",
        "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "            while not done:\n",
        "                # Agent-specific action selection\n",
        "                if isinstance(agent, DQNAgent):\n",
        "                    action = agent.act(state)\n",
        "                elif isinstance(agent, A2CAgent):\n",
        "                    action, prob = agent.act(state)\n",
        "                elif isinstance(agent, PPOAgent):\n",
        "                    action, prob = agent.act(state)\n",
        "\n",
        "                # Environment step\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                episode_info.append(info)\n",
        "\n",
        "                # Store experience\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                next_states.append(next_state)\n",
        "                dones.append(int(done))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                # Agent-specific updates\n",
        "                if isinstance(agent, DQNAgent):\n",
        "                    agent.remember(state, action, reward, next_state, done)\n",
        "                    if len(agent.memory) > agent.batch_size:\n",
        "                        agent.replay()\n",
        "\n",
        "                    if episode % agent.target_update == 0:\n",
        "                        agent.update_target_network()\n",
        "\n",
        "                elif isinstance(agent, A2CAgent):\n",
        "                    if done:\n",
        "                        agent.update(states, actions, rewards, next_states, dones)\n",
        "\n",
        "                elif isinstance(agent, PPOAgent):\n",
        "                    if done:\n",
        "                        old_probs = [prob] * len(states)  # Placeholder for demo\n",
        "                        agent.update(states, actions, old_probs, rewards, next_states, dones)\n",
        "\n",
        "            # Track metrics\n",
        "            metrics_tracker.add_episode_data(episode_info)\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % 50 == 0:\n",
        "              total_reward = sum(rewards)\n",
        "              print(f\"{agent_name} Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "\n",
        "        return metrics_tracker\n",
        "\n",
        "    # Train each agent\n",
        "    dqn_results = train_agent(dqn_agent, dqn_metrics, \"DQN\")\n",
        "    a2c_results = train_agent(a2c_agent, a2c_metrics, \"A2C\")\n",
        "    ppo_results = train_agent(ppo_agent, ppo_metrics, \"PPO\")\n",
        "\n",
        "    # Comparative Visualization\n",
        "    plt.figure(figsize=(20, 12))\n",
        "    # Rewards Comparison\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(dqn_results.episode_rewards, label='DQN')\n",
        "    plt.plot(a2c_results.episode_rewards, label='A2C')\n",
        "    plt.plot(ppo_results.episode_rewards, label='PPO')\n",
        "    plt.title('Episode Rewards Comparison')\n",
        "    plt.legend()\n",
        "\n",
        "    # Relevance Scores Comparison\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.boxplot([\n",
        "        dqn_results.relevance_scores,\n",
        "        a2c_results.relevance_scores,\n",
        "        ppo_results.relevance_scores\n",
        "    ], labels=['DQN', 'A2C', 'PPO'])\n",
        "    plt.title('Relevance Scores Distribution')\n",
        "\n",
        "    # CTR Comparison\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.boxplot([\n",
        "        dqn_results.ctr_values,\n",
        "        a2c_results.ctr_values,\n",
        "        ppo_results.ctr_values\n",
        "    ], labels=['DQN', 'A2C', 'PPO'])\n",
        "    plt.title('CTR Distribution')\n",
        "\n",
        "    # Final Metrics Summary\n",
        "    plt.subplot(2, 3, 4)\n",
        "    metrics = {\n",
        "        'DQN': dqn_results.calculate_metrics(),\n",
        "        'A2C': a2c_results.calculate_metrics(),\n",
        "        'PPO': ppo_results.calculate_metrics()\n",
        "    }\n",
        "\n",
        "    # Create table-like visualization\n",
        "    plt.text(0.1, 0.9, 'Metrics Comparison', fontsize=12, fontweight='bold')\n",
        "    y_pos = 0.8\n",
        "    for agent, metric_dict in metrics.items():\n",
        "        plt.text(0.1, y_pos, f\"{agent}:\", fontweight='bold')\n",
        "        for key, value in list(metric_dict.items())[:5]:\n",
        "            y_pos -= 0.1\n",
        "            plt.text(0.2, y_pos, f\"{key}: {value:.4f}\")\n",
        "        y_pos -= 0.1\n",
        "\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed summaries\n",
        "    print(\"\\n=== DQN Performance Summary ===\")\n",
        "    dqn_results.print_summary()\n",
        "\n",
        "    print(\"\\n=== A2C Performance Summary ===\")\n",
        "    a2c_results.print_summary()\n",
        "\n",
        "    print(\"\\n=== PPO Performance Summary ===\")\n",
        "    ppo_results.print_summary()\n",
        "\n",
        "    return dqn_results, a2c_results, ppo_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Run training\n",
        "    dqn_results, a2c_results, ppo_results = train_agents(episodes=500)"
      ],
      "metadata": {
        "id": "omC1dz9ZVvdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"DQN Rewards Length: {len(dqn_results.episode_rewards)}\")\n",
        "print(f\"A2C Rewards Length: {len(a2c_results.episode_rewards)}\")\n",
        "print(f\"PPO Rewards Length: {len(ppo_results.episode_rewards)}\")\n",
        "\n",
        "min_len = min(len(dqn_results.episode_rewards),\n",
        "              len(a2c_results.episode_rewards),\n",
        "              len(ppo_results.episode_rewards))\n",
        "\n",
        "print(f\"Minimum Length for Visualization: {min_len}\")\n"
      ],
      "metadata": {
        "id": "9TmrQglpDJGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agents(episodes=500):\n",
        "    # Initialize environment\n",
        "    env = AdPlacementEnvironment(n_slots=3)\n",
        "    state_size = len(env.reset())\n",
        "    action_size = len(env.ads_df)\n",
        "\n",
        "    # Initialize agents\n",
        "    dqn_agent = DQNAgent(state_size, action_size)\n",
        "    a2c_agent = A2CAgent(state_size, action_size)\n",
        "    ppo_agent = PPOAgent(state_size, action_size)\n",
        "\n",
        "    # Metrics trackers\n",
        "    dqn_metrics = AdvancedMetrics()\n",
        "    a2c_metrics = AdvancedMetrics()\n",
        "    ppo_metrics = AdvancedMetrics()\n",
        "\n",
        "    # Training loop for each agent\n",
        "    def train_agent(agent, metrics_tracker, agent_name):\n",
        "        print(f\"\\n=== Training {agent_name} Agent ===\")\n",
        "        for episode in range(episodes):\n",
        "            state = env.reset()\n",
        "            done = False\n",
        "            episode_info = []\n",
        "            states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "            while not done:\n",
        "                # Agent-specific action selection\n",
        "                if isinstance(agent, DQNAgent):\n",
        "                    action = agent.act(state)\n",
        "                elif isinstance(agent, A2CAgent):\n",
        "                    action, prob = agent.act(state)\n",
        "                elif isinstance(agent, PPOAgent):\n",
        "                    action, prob = agent.act(state)\n",
        "\n",
        "                # Environment step\n",
        "                next_state, reward, done, info = env.step(action)\n",
        "                episode_info.append(info)\n",
        "\n",
        "                # Store experience\n",
        "                states.append(state)\n",
        "                actions.append(action)\n",
        "                rewards.append(reward)\n",
        "                next_states.append(next_state)\n",
        "                dones.append(int(done))\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "                # Agent-specific updates\n",
        "                if isinstance(agent, DQNAgent):\n",
        "                    agent.remember(state, action, reward, next_state, done)\n",
        "                    if len(agent.memory) > agent.batch_size:\n",
        "                        agent.replay()\n",
        "\n",
        "                    if episode % agent.target_update == 0:\n",
        "                        agent.update_target_network()\n",
        "\n",
        "                elif isinstance(agent, A2CAgent):\n",
        "                    if done:\n",
        "                        agent.update(states, actions, rewards, next_states, dones)\n",
        "\n",
        "                elif isinstance(agent, PPOAgent):\n",
        "                    if done:\n",
        "                        old_probs = [prob] * len(states)  # Placeholder for demo\n",
        "                        agent.update(states, actions, old_probs, rewards, next_states, dones)\n",
        "\n",
        "            # Track metrics\n",
        "            metrics_tracker.add_episode_data(episode_info)\n",
        "\n",
        "            # Print progress\n",
        "            if (episode + 1) % 50 == 0:\n",
        "                total_reward = sum(rewards)\n",
        "                print(f\"{agent_name} Episode {episode + 1}/{episodes} | Total Reward: {total_reward}\")\n",
        "\n",
        "        return metrics_tracker\n",
        "\n",
        "    # Train each agent\n",
        "    dqn_results = train_agent(dqn_agent, dqn_metrics, \"DQN\")\n",
        "    a2c_results = train_agent(a2c_agent, a2c_metrics, \"A2C\")\n",
        "    ppo_results = train_agent(ppo_agent, ppo_metrics, \"PPO\")\n",
        "\n",
        "    # Comparative Visualization\n",
        "    plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # Rewards Comparison\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(dqn_results.episode_rewards, label='DQN')\n",
        "    plt.plot(a2c_results.episode_rewards, label='A2C')\n",
        "    plt.plot(ppo_results.episode_rewards, label='PPO')\n",
        "    plt.title('Episode Rewards Comparison')\n",
        "    plt.legend()\n",
        "\n",
        "    # Relevance Scores Comparison\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.boxplot([\n",
        "        dqn_results.relevance_scores,\n",
        "        a2c_results.relevance_scores,\n",
        "        ppo_results.relevance_scores\n",
        "    ], labels=['DQN', 'A2C', 'PPO'])\n",
        "    plt.title('Relevance Scores Distribution')\n",
        "\n",
        "    # CTR Comparison\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.boxplot([\n",
        "        dqn_results.ctr_values,\n",
        "        a2c_results.ctr_values,\n",
        "        ppo_results.ctr_values\n",
        "    ], labels=['DQN', 'A2C', 'PPO'])\n",
        "    plt.title('CTR Distribution')\n",
        "\n",
        "    # Additional Plots\n",
        "    # 1. Conversion Rate Comparison\n",
        "    plt.subplot(2, 3, 4)\n",
        "    plt.boxplot([\n",
        "        dqn_results.conversion_rates,\n",
        "        a2c_results.conversion_rates,\n",
        "        ppo_results.conversion_rates\n",
        "    ], labels=['DQN', 'A2C', 'PPO'])\n",
        "    plt.title('Conversion Rates Comparison')\n",
        "\n",
        "    # 2. Moving Average of Rewards for All Agents\n",
        "    plt.subplot(2, 3, 5)\n",
        "    window_size = max(1, episodes // 20)\n",
        "    plt.plot(pd.Series(dqn_results.episode_rewards).rolling(window=window_size).mean(), label='DQN')\n",
        "    plt.plot(pd.Series(a2c_results.episode_rewards).rolling(window=window_size).mean(), label='A2C')\n",
        "    plt.plot(pd.Series(ppo_results.episode_rewards).rolling(window=window_size).mean(), label='PPO')\n",
        "    plt.title(f'Moving Average of Rewards (window={window_size})')\n",
        "    plt.legend()\n",
        "\n",
        "    # 3. Unique Ads Used Comparison\n",
        "    plt.subplot(2, 3, 6)\n",
        "    unique_ads = [\n",
        "        len(set(dqn_results.placement_history)),\n",
        "        len(set(a2c_results.placement_history)),\n",
        "        len(set(ppo_results.placement_history))\n",
        "    ]\n",
        "    plt.bar(['DQN', 'A2C', 'PPO'], unique_ads)\n",
        "    plt.title('Unique Ads Used Comparison')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed summaries\n",
        "    print(\"\\n=== DQN Performance Summary ===\")\n",
        "    dqn_results.print_summary()\n",
        "\n",
        "    print(\"\\n=== A2C Performance Summary ===\")\n",
        "    a2c_results.print_summary()\n",
        "\n",
        "    print(\"\\n=== PPO Performance Summary ===\")\n",
        "    ppo_results.print_summary()\n",
        "\n",
        "    return dqn_results, a2c_results, ppo_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Run training\n",
        "    dqn_results, a2c_results, ppo_results = train_agents(episodes=500)\n"
      ],
      "metadata": {
        "id": "PqMYZr_1Y117"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dqn_results.episode_rewards))  # Should print a positive number\n",
        "print(len(a2c_results.episode_rewards))  # Should print a positive number\n",
        "print(len(ppo_results.episode_rewards))  # Should print a positive number\n",
        "print(dqn_results.episode_rewards[:5])  # First 5 rewards for DQN\n",
        "print(a2c_results.episode_rewards[:5])   # First 5 rewards for A2C\n",
        "print(ppo_results.episode_rewards[:5])   # First 5 rewards for PPO"
      ],
      "metadata": {
        "id": "W32caRv1BfNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def advanced_visualizations(dqn_results, a2c_results, ppo_results):\n",
        "    \"\"\"\n",
        "    Generate advanced visualizations for DRL agent performance comparison.\n",
        "    \"\"\"\n",
        "    # Get the minimum length of the episode rewards across all agents\n",
        "    min_len = min(len(dqn_results.episode_rewards),\n",
        "                  len(a2c_results.episode_rewards),\n",
        "                  len(ppo_results.episode_rewards))\n",
        "\n",
        "    # Convert data to a DataFrame for Seaborn, ensuring equal lengths\n",
        "    data = {\n",
        "        \"Agent\": [\"DQN\"] * min_len + [\"A2C\"] * min_len + [\"PPO\"] * min_len,\n",
        "        \"Episode\": list(range(1, min_len + 1)) * 3,\n",
        "        \"Rewards\": dqn_results.episode_rewards[:min_len] +\n",
        "                   a2c_results.episode_rewards[:min_len] +\n",
        "                   ppo_results.episode_rewards[:min_len],\n",
        "        \"CTR\": dqn_results.ctr_values[:min_len] +\n",
        "               a2c_results.ctr_values[:min_len] +\n",
        "               ppo_results.ctr_values[:min_len],\n",
        "        \"Relevance\": dqn_results.relevance_scores[:min_len] +\n",
        "                     a2c_results.relevance_scores[:min_len] +\n",
        "                     ppo_results.relevance_scores[:min_len]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # 1. Heatmap of Reward Distribution Over Episodes\n",
        "    pivot_rewards = df.pivot(index=\"Episode\", columns=\"Agent\", values=\"Rewards\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_rewards, cmap=\"viridis\", annot=False, cbar_kws={'label': 'Reward'}, linewidths=0.5)\n",
        "    plt.title(\"Heatmap of Rewards Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Agent\", fontsize=14)\n",
        "    plt.ylabel(\"Episode\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. KDE Plot of Reward Distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"DQN\"][\"Rewards\"], label=\"DQN\", fill=True)\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"A2C\"][\"Rewards\"], label=\"A2C\", fill=True)\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"PPO\"][\"Rewards\"], label=\"PPO\", fill=True)\n",
        "    plt.title(\"Kernel Density Estimation (KDE) of Reward Distributions\", fontsize=16)\n",
        "    plt.xlabel(\"Rewards\", fontsize=14)\n",
        "    plt.ylabel(\"Density\", fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Line Plot of Cumulative Rewards\n",
        "    cumulative_rewards = df.groupby(\"Agent\")[\"Rewards\"].cumsum().reset_index()\n",
        "    cumulative_rewards = cumulative_rewards.rename(columns={\"Rewards\": \"Cumulative Rewards\"})\n",
        "    df = df.merge(cumulative_rewards, left_index=True, right_index=True)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Episode\", y=\"Cumulative Rewards\", hue=\"Agent\", marker=\"o\")\n",
        "    plt.title(\"Cumulative Rewards Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Episode\", fontsize=14)\n",
        "    plt.ylabel(\"Cumulative Rewards\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Pair Plot for Performance Metrics\n",
        "    sns.pairplot(df, vars=[\"Rewards\", \"CTR\", \"Relevance\"], hue=\"Agent\", diag_kind=\"kde\", palette=\"deep\")\n",
        "    plt.suptitle(\"Pairwise Comparison of Metrics\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Violin Plot for Relevance Scores Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.violinplot(data=df, x=\"Agent\", y=\"Relevance\", palette=\"muted\", scale=\"width\")\n",
        "    plt.title(\"Relevance Score Distribution Across Agents\", fontsize=16)\n",
        "    plt.xlabel(\"Agent\", fontsize=14)\n",
        "    plt.ylabel(\"Relevance Score\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example Call\n",
        "advanced_visualizations(dqn_results, a2c_results, ppo_results)\n"
      ],
      "metadata": {
        "id": "9_fs38kIdyIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np  # Ensure NumPy is imported\n",
        "\n",
        "def advanced_visualizations(dqn_results, a2c_results, ppo_results):\n",
        "    \"\"\"\n",
        "    Generate advanced visualizations for DRL agent performance comparison, including additional metrics.\n",
        "    \"\"\"\n",
        "    # Get the minimum length of the episode rewards across all agents\n",
        "    min_len = min(len(dqn_results.episode_rewards),\n",
        "                  len(a2c_results.episode_rewards),\n",
        "                  len(ppo_results.episode_rewards))\n",
        "\n",
        "    # Check if min_len is 0 and handle the case\n",
        "    if min_len == 0:\n",
        "        print(\"No episode rewards data to visualize. Please ensure the agents have been trained.\")\n",
        "        return\n",
        "\n",
        "    # Convert data to a DataFrame for Seaborn, ensuring equal lengths\n",
        "    data = {\n",
        "        \"Agent\": [\"DQN\"] * min_len + [\"A2C\"] * min_len + [\"PPO\"] * min_len,\n",
        "        \"Episode\": list(range(1, min_len + 1)) * 3,\n",
        "        \"Rewards\": dqn_results.episode_rewards[:min_len] +\n",
        "                   a2c_results.episode_rewards[:min_len] +\n",
        "                   ppo_results.episode_rewards[:min_len],\n",
        "        \"CTR\": dqn_results.ctr_values[:min_len] +\n",
        "               a2c_results.ctr_values[:min_len] +\n",
        "               ppo_results.ctr_values[:min_len],\n",
        "        \"Relevance\": dqn_results.relevance_scores[:min_len] +\n",
        "                     a2c_results.relevance_scores[:min_len] +\n",
        "                     ppo_results.relevance_scores[:min_len],\n",
        "        \"Conversion Rate\": dqn_results.conversion_rates[:min_len] +\n",
        "                           a2c_results.conversion_rates[:min_len] +\n",
        "                           ppo_results.conversion_rates[:min_len],\n",
        "        \"Exploration vs Exploitation\": dqn_results.exploration_vs_exploitation[:min_len] +\n",
        "                                       a2c_results.exploration_vs_exploitation[:min_len] +\n",
        "                                       ppo_results.exploration_vs_exploitation[:min_len],\n",
        "        \"Success Rate\": dqn_results.success_rates[:min_len] +  # Corrected 'success_rate' to 'success_rates'\n",
        "                        a2c_results.success_rates[:min_len] +  # Corrected 'success_rate' to 'success_rates'\n",
        "                        ppo_results.success_rates[:min_len]  # Corrected 'success_rate' to 'success_rates'\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # 1. Heatmap of Reward Distribution Over Episodes\n",
        "    pivot_rewards = df.pivot(index=\"Episode\", columns=\"Agent\", values=\"Rewards\")\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(pivot_rewards, cmap=\"viridis\", annot=False, cbar_kws={'label': 'Reward'}, linewidths=0.5)\n",
        "    plt.title(\"Heatmap of Rewards Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Agent\", fontsize=14)\n",
        "    plt.ylabel(\"Episode\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. KDE Plot of Reward Distributions\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"DQN\"][\"Rewards\"], label=\"DQN\", fill=True)\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"A2C\"][\"Rewards\"], label=\"A2C\", fill=True)\n",
        "    sns.kdeplot(df[df[\"Agent\"] == \"PPO\"][\"Rewards\"], label=\"PPO\", fill=True)\n",
        "    plt.title(\"Kernel Density Estimation (KDE) of Reward Distributions\", fontsize=16)\n",
        "    plt.xlabel(\"Rewards\", fontsize=14)\n",
        "    plt.ylabel(\"Density\", fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Line Plot of Click-Through Rate (CTR)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Episode\", y=\"CTR\", hue=\"Agent\", marker=\"o\")\n",
        "    plt.title(\"Click-Through Rate Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Episode\", fontsize=14)\n",
        "    plt.ylabel(\"CTR\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Line Plot of Conversion Rate\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Episode\", y=\"Conversion Rate\", hue=\"Agent\", marker=\"o\")\n",
        "    plt.title(\"Conversion Rate Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Episode\", fontsize=14)\n",
        "    plt.ylabel(\"Conversion Rate\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 5. Line Plot of Exploration vs. Exploitation Ratio\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Episode\", y=\"Exploration vs Exploitation\", hue=\"Agent\", marker=\"o\")\n",
        "    plt.title(\"Exploration vs. Exploitation Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Episode\", fontsize=14)\n",
        "    plt.ylabel(\"Exploration vs Exploitation Ratio\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 6. Line Plot of Success Rate\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(data=df, x=\"Episode\", y=\"Success Rate\", hue=\"Agent\", marker=\"o\")\n",
        "    plt.title(\"Success Rate Over Episodes\", fontsize=16)\n",
        "    plt.xlabel(\"Episode\", fontsize=14)\n",
        "    plt.ylabel(\"Success Rate\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 7. Pair Plot for All Metrics\n",
        "    sns.pairplot(df, vars=[\"Rewards\", \"CTR\", \"Conversion Rate\", \"Exploration vs Exploitation\", \"Success Rate\"],\n",
        "                 hue=\"Agent\", diag_kind=\"kde\", palette=\"deep\")\n",
        "    plt.suptitle(\"Pairwise Comparison of Metrics\", y=1.02, fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "    # 8. Violin Plot for Relevance Scores Distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.violinplot(data=df, x=\"Agent\", y=\"Relevance\", palette=\"muted\", scale=\"width\")\n",
        "    plt.title(\"Relevance Score Distribution Across Agents\", fontsize=16)\n",
        "    plt.xlabel(\"Agent\", fontsize=14)\n",
        "    plt.ylabel(\"Relevance Score\", fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Example of how to initialize and use the class\n",
        "dqn_results = AdvancedMetrics()\n",
        "a2c_results = AdvancedMetrics()\n",
        "ppo_results = AdvancedMetrics()\n",
        "\n",
        "\n",
        "# Now you can call the visualization function\n",
        "advanced_visualizations(dqn_results, a2c_results, ppo_results)\n"
      ],
      "metadata": {
        "id": "Ut4MXaBggmoq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}